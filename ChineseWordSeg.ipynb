{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Chinese Word Segmentation\n",
    "========================\n",
    "\n",
    "In this notebook, I applied the method of \"*word  boundary  decision  (WBD)  segmentation*\" model based on the paper [Huang, Chu-Ren, et al. \"A realistic and robust model for Chinese word segmentation.\" arXiv preprint arXiv:1905.08732 (2019)](https://arxiv.org/pdf/1905.08732.pdf) for Chinese word segmentation.\n",
    "\n",
    "I followed the method mentioned in \"ChineseWordSegmentation.pdf\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> All the essential functions are defined in the \"util_funcs.py\".\n",
    "</div>\n",
    "These functions are imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from scipy import sparse\n",
    "import util_funcs as uf  # essential functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "`n_gram`: number of the consecutive characters considered eg. `n_gram`=4 we will have *树高遭雷*.\n",
    "\n",
    "`vec_size`: size of the feature vector. In this approach the feature vector defined for the example above will be  *\\[树高, 高, 高遭, 遭, 遭雷\\]*.\n",
    "\n",
    "`Path2Data`: Path to the input data\n",
    "\n",
    "`FileTrain`/`FileTest`: Name of the file (training/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 4 \n",
    "vec_size = 5  \n",
    "Path2Data = './data/'\n",
    "FileTrain = 'training.txt'\n",
    "FileTest = 'test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:There were some decoding erros in the files. I ignored those characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating feature vector and labels of training and test dataset\n",
    "\n",
    "`corpus`: A dictionary containing all the uni-gram and bi-grams of the characters in the training dataset. <div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> I added a \"oov\" to the corpus to capture the out-of-vocabulary in the test dataset. Whenever an unseen word shows up in the test dataset, the index of \"oov\" will be used.\n",
    "</div>\n",
    "\n",
    "`X_train`, `X_test`: Feature vector of traiing/test dataset\n",
    "\n",
    "`Y_train`, `Y_test`: Labels of training/test dataset. The values are 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data set\n",
    "corpus, X_train, Y_train = uf.Train_XY(Path2Data, FileTrain)\n",
    "\n",
    "# Save the sparse matrix to save time for future!\n",
    "sparse.save_npz(\"SparseTrain.npz\", X_train)\n",
    "\n",
    "# Load the sparse matrix for training\n",
    "# X_train = sparse.load_npz(\"SparseTrain.npz\")\n",
    "\n",
    "# Test dataset\n",
    "X_test, Y_test = uf.Test_XY(Path2Data, FileTest, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Since `X_train` is very sparse, to increase the computation time I used a Bernouli Naive Bayes method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=None, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classification based on Bernoulli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB(binarize=None)\n",
    "nb.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.82034132841329% \n",
      "\n",
      "f1 score: 94.0643462821808% \n",
      "\n",
      "precision score: 90.16431924882629% \n",
      "\n",
      "recall score: 98.31701542202597% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "Y_predict = nb.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(Y_test, Y_predict)*100}% \\n')\n",
    "\n",
    "print(f'f1 score: {f1_score(Y_test, Y_predict)*100}% \\n')\n",
    "\n",
    "print(f'precision score: {precision_score(Y_test, Y_predict)*100}% \\n')\n",
    "\n",
    "print(f'recall score: {recall_score(Y_test, Y_predict)*100}% \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
